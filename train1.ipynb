{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_McZcxZzVbdQ",
        "outputId": "1ebb5df5-b762-4970-82ed-b2b02dce35ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Io7jWrGWU-lq"
      },
      "outputs": [],
      "source": [
        "# Load all required library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import sys\n",
        "# sys.path.append('/Users/jerawincel/anaconda3/lib/python3.10/site-packages')\n",
        "import nltk\n",
        "\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import sklearn\n",
        "import contractions\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcHSWaiCWjRU",
        "outputId": "42493f28-48a9-4074-f17d-7c91f0eaf722"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRjavniqVrhN",
        "outputId": "5ba5b0fb-28d4-4904-b679-50dfb47e71d4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/CSCI544/amazon_reviews_us_Office_Products_v1_00.tsv'"
      ],
      "metadata": {
        "id": "SRQN68EKVyaL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data as a Pandas frame using Pandas\n",
        "# path = '/Users/jerawincel/Documents/USC/CSCI544/HW1/amazon_reviews_us_Office_Products_v1_00.tsv'\n",
        "raw_data = pd.read_table(path, on_bad_lines='skip')\n",
        "\n",
        "new_data = raw_data[['star_rating', 'review_headline', 'review_body']].copy()\n",
        "# combine review headline and review body for review\n",
        "new_data.loc[:, 'review'] = new_data['review_headline'] + ' ' + new_data['review_body']\n",
        "new_data = new_data[['star_rating', 'review']]\n",
        "\n",
        "# remove emoji in reviews\n",
        "def handle_emoji(string):\n",
        "    if isinstance(string, str):\n",
        "        emoji = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\" # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "        return emoji.sub(r'', string)\n",
        "    else:\n",
        "        return string\n",
        "\n",
        "new_data['review'] = new_data['review'].apply(handle_emoji)\n",
        "# drop entries with null values\n",
        "new_data = new_data.dropna()\n",
        "\n",
        "new_data['class'] = 0\n",
        "# Let ratings with the values of 1, 2 and 3 form class 1\n",
        "new_data.loc[new_data['star_rating'].isin([1, 2, 3]), 'class'] = 1\n",
        "# Let ratings with the values of 4 and 5 form class 2\n",
        "new_data.loc[new_data['star_rating'].isin([4, 5]), 'class'] = 2\n",
        "# Delete entries with invalid ratings\n",
        "new_data = new_data[new_data['class'] != 0]\n",
        "\n",
        "# randomly select 50000 reviews from each class\n",
        "class1 = new_data[new_data['class'] == 1].sample(n = 50000, random_state=1, ignore_index=True)\n",
        "class2 = new_data[new_data['class'] == 2].sample(n = 50000, random_state=1, ignore_index=True)\n",
        "samples = pd.concat([class1, class2], axis=0).reset_index(drop=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmRXc6o3VW_r",
        "outputId": "017f0980-bac6-45a7-996a-81e8a9401ab5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-b2473dd718f9>:3: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  raw_data = pd.read_table(path, on_bad_lines='skip')\n",
            "<ipython-input-8-b2473dd718f9>:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  new_data['class'] = 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Cleaning\n",
        "# get average length of the reviews in terms of character length before cleaning\n",
        "len_before_clean = samples['review'].str.len().mean()\n",
        "\n",
        "# convert all reviews into lowercase\n",
        "X_clean = samples['review'].str.lower()\n",
        "# remove the HTML and URLs from the reviews\n",
        "def remove_html(review):\n",
        "    if isinstance(review, str):\n",
        "        soup = BeautifulSoup(review, 'html.parser')\n",
        "        text = soup.get_text()\n",
        "        return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    else:\n",
        "        return review\n",
        "\n",
        "X_clean = X_clean.apply(remove_html)\n",
        "# remove non-alphabetical characters\n",
        "X_clean = X_clean.str.replace(r'[^a-z\\s]', '',regex=True)\n",
        "\n",
        "# remove extra spaces\n",
        "def remove_spaces(string):\n",
        "    return ' '.join(string.split())\n",
        "\n",
        "X_clean = X_clean.apply(remove_spaces)\n",
        "# perform contractions on the reviews\n",
        "def perform_contractions(string):\n",
        "    return contractions.fix(string)\n",
        "\n",
        "X_clean = X_clean.apply(perform_contractions)\n",
        "\n",
        "# get average length of the reviews in terms of character length after cleaning\n",
        "len_after_clean = X_clean.str.len().mean()\n",
        "\n",
        "# print average length of reviews before and after data cleaning\n",
        "print(f'{len_before_clean:.2f}, {len_after_clean:.2f}')\n",
        "\n",
        "# Data Preprocessing\n",
        "\n",
        "# remove stop words\n",
        "def remove_stop_words(review):\n",
        "    token = nltk.tokenize.word_tokenize(review)\n",
        "    filtered_token = [w for w in token if w not in stopwords.words('english')]\n",
        "    return ' '.join(filtered_token)\n",
        "\n",
        "X_clean = X_clean.apply(remove_stop_words)\n",
        "\n",
        "# lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatization(review):\n",
        "    token = nltk.tokenize.word_tokenize(review)\n",
        "    lemmatized_words = [lemmatizer.lemmatize(w) for w in token]\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "X_clean = X_clean.apply(lemmatization)\n",
        "\n",
        "# get average length of the reviews in terms of character length after preprocessing\n",
        "len_after_pre = X_clean.str.len().mean()\n",
        "\n",
        "# print average length of reviews before and after data preprocessing\n",
        "print(f'{len_after_clean:.2f}, {len_after_pre:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qM2cDSH0Wpcz",
        "outputId": "de3fd56f-cc58-47ee-cb34-7a17d5a222ab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-0591bb9f9f92>:10: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(review, 'html.parser')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "347.04, 329.53\n",
            "329.53, 207.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split data into training and testing dataset\n",
        "X_train_clean, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_clean, samples['class'], test_size=0.2, random_state=7)\n"
      ],
      "metadata": {
        "id": "NDM9C6MmXC1Q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feature extraction\n",
        "# TF-IDF Feature Extraction\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_clean)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Bag of Words (BoW) feature extraction\n",
        "bow_vectorizer = CountVectorizer()\n",
        "X_train_bow = bow_vectorizer.fit_transform(X_train_clean)\n",
        "X_test_bow = bow_vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "0ZL3rGALWFWb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perceptron\n",
        "perceptron = Perceptron(random_state=0)\n",
        "\n",
        "# train a perceptron model using bow feature\n",
        "perceptron.fit(X_train_bow, y_train)\n",
        "preceptron_predict = perceptron.predict(X_test_bow)\n",
        "\n",
        "# Report Precision, Recall, and f1-score for BoW features.\n",
        "bow_accuracy = accuracy_score(y_test, preceptron_predict)\n",
        "bow_precision = precision_score(y_test, preceptron_predict)\n",
        "bow_recall = recall_score(y_test, preceptron_predict)\n",
        "bow_f1 = f1_score(y_test, preceptron_predict)\n",
        "\n",
        "# print('Report for training Perceptron model using BoW features:')\n",
        "# print(f'Accuracy: {bow_accuracy:.2f}; Precision: {bow_precision:.2f}; Recall: {bow_recall:.2f}; F1: {bow_f1:.2f}')\n",
        "print(f'{bow_precision:.3f} {bow_recall:.3f} {bow_f1:.3f}')\n",
        "\n",
        "\n",
        "# train a perceptron model for tfidf\n",
        "perceptron.fit(X_train_tfidf, y_train)\n",
        "preceptron_predict = perceptron.predict(X_test_tfidf)\n",
        "\n",
        "# Report Precision, Recall, and f1-score\n",
        "tfidf_accuracy = accuracy_score(y_test, preceptron_predict)\n",
        "tfidf_precision = precision_score(y_test, preceptron_predict)\n",
        "tfidf_recall = recall_score(y_test, preceptron_predict)\n",
        "tfidf_f1 = f1_score(y_test, preceptron_predict)\n",
        "\n",
        "# print('Report for training Perceptron model using TFIDF features:')\n",
        "# print(f'Accuracy: {tfidf_accuracy:.2f}; Precision: {tfidf_precision:.2f}; Recall: {tfidf_recall:.2f}; F1: {tfidf_f1:.2f}')\n",
        "print(f'{tfidf_precision:.3f} {tfidf_recall:.3f} {tfidf_f1:.3f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqpLRiWgWThi",
        "outputId": "583f8a37-4f04-4718-ba7b-87e77f121a65"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.855 0.823 0.839\n",
            "0.859 0.809 0.834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# svm\n",
        "svm_model = LinearSVC(random_state=0, max_iter=100000, tol=1e-6, dual=True)\n",
        "\n",
        "# train svm model using bow features\n",
        "svm_model.fit(X_train_bow, y_train)\n",
        "svm_pred = svm_model.predict(X_test_bow)\n",
        "\n",
        "# Report Precision, Recall, and f1-score for training SVM using BoW features.\n",
        "bow_accuracy = accuracy_score(y_test, svm_pred)\n",
        "bow_precision = precision_score(y_test, svm_pred)\n",
        "bow_recall = recall_score(y_test, svm_pred)\n",
        "bow_f1 = f1_score(y_test, svm_pred)\n",
        "\n",
        "# print('Report for training SVM model using BoW features:')\n",
        "print(f'{bow_precision:.3f} {bow_recall:.3f} {bow_f1:.3f}')\n",
        "\n",
        "# train svm model using tfidf features\n",
        "svm_model.fit(X_train_tfidf, y_train)\n",
        "svm_pred = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "# Report Precision, Recall, and f1-score for training SVM using tfidf features.\n",
        "tfidf_accuracy = accuracy_score(y_test, svm_pred)\n",
        "tfidf_precision = precision_score(y_test, svm_pred)\n",
        "tfidf_recall = recall_score(y_test, svm_pred)\n",
        "tfidf_f1 = f1_score(y_test, svm_pred)\n",
        "\n",
        "# print('Report for training SVM model using TFIDF features:')\n",
        "print(f'{tfidf_precision:.3f} {tfidf_recall:.3f} {tfidf_f1:.3f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtQjlwX5XOxm",
        "outputId": "4267abd6-6e86-44e7-f841-f7b77e645962"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.863 0.864 0.863\n",
            "0.878 0.887 0.882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logistic regression\n",
        "logistic_model = LogisticRegression(random_state=0, max_iter=10000)\n",
        "\n",
        "# train logistic regression model using bow features\n",
        "logistic_model.fit(X_train_bow, y_train)\n",
        "logistic_pred = logistic_model.predict(X_test_bow)\n",
        "\n",
        "# Report Precision, Recall, and f1-score for training Logistic regression using BoW features.\n",
        "bow_accuracy = accuracy_score(y_test, logistic_pred)\n",
        "bow_precision = precision_score(y_test, logistic_pred)\n",
        "bow_recall = recall_score(y_test, logistic_pred)\n",
        "bow_f1 = f1_score(y_test, logistic_pred)\n",
        "\n",
        "# print('Report for training Logistic Regression model using BoW features:')\n",
        "print(f'{bow_precision:.3f} {bow_recall:.3f} {bow_f1:.3f}')\n",
        "\n",
        "# train logistic regression model using tfidf features\n",
        "logistic_model.fit(X_train_tfidf, y_train)\n",
        "logistic_pred = logistic_model.predict(X_test_tfidf)\n",
        "\n",
        "# Report Precision, Recall, and f1-score for training Logistic regression using tfidf features.\n",
        "tfidf_accuracy = accuracy_score(y_test, logistic_pred)\n",
        "tfidf_precision = precision_score(y_test, logistic_pred)\n",
        "tfidf_recall = recall_score(y_test, logistic_pred)\n",
        "tfidf_f1 = f1_score(y_test, logistic_pred)\n",
        "\n",
        "# print('Report for training Logistic Regression model using TFIDF features:')\n",
        "print(f'{tfidf_precision:.3f} {tfidf_recall:.3f} {tfidf_f1:.3f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVCVzicMXSP2",
        "outputId": "3884ad13-bc6e-4f14-d2a5-d4a344fb1698"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.879 0.876 0.877\n",
            "0.880 0.895 0.887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# naive bayes\n",
        "nb_model = MultinomialNB(force_alpha=True)\n",
        "\n",
        "# train naive bayes model using bow features\n",
        "nb_model.fit(X_train_bow, y_train)\n",
        "nb_pred = nb_model.predict(X_test_bow)\n",
        "\n",
        "# Report Precision, Recall, and f1-score for training naive bayes using BoW features.\n",
        "bow_accuracy = accuracy_score(y_test, nb_pred)\n",
        "bow_precision = precision_score(y_test, nb_pred)\n",
        "bow_recall = recall_score(y_test, nb_pred)\n",
        "bow_f1 = f1_score(y_test, nb_pred)\n",
        "\n",
        "# print('Report for training Naive Bayes model using BoW features:')\n",
        "print(f'{bow_precision:.3f} {bow_recall:.3f} {bow_f1:.3f}')\n",
        "\n",
        "# train naive bayes model using bow features\n",
        "nb_model.fit(X_train_tfidf, y_train)\n",
        "nb_pred = nb_model.predict(X_test_tfidf)\n",
        "\n",
        "# Report Precision, Recall, and f1-score for training naive bayes using BoW features.\n",
        "tfidf_accuracy = accuracy_score(y_test, nb_pred)\n",
        "tfidf_precision = precision_score(y_test, nb_pred)\n",
        "tfidf_recall = recall_score(y_test, nb_pred)\n",
        "tfidf_f1 = f1_score(y_test, nb_pred)\n",
        "\n",
        "# print('Report for training Naive Bayes model using TFIDF features:')\n",
        "print(f'{tfidf_precision:.3f} {tfidf_recall:.3f} {tfidf_f1:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXI4RH-kV--T",
        "outputId": "796b4256-1d3b-457f-c80e-8ee28c1eabca"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.862 0.798 0.828\n",
            "0.821 0.885 0.852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5YhWWx2VdZqN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}