{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "lNW-5HQyd_Eh",
        "outputId": "3cedb84e-7aed-4f8c-c1e9-68208feabf52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Load all required library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "enU5_7aRfxJB",
        "outputId": "aade8ee5-4866-4d85-b12a-726d3470622a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukHt_2mof3L1"
      },
      "source": [
        "# 1. Dataset Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KYSIzbVrfyzh",
        "outputId": "e83c9887-ac48-4e95-b71a-1b2756df82a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-5b6687d1ed64>:4: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  raw_data = pd.read_table(path, on_bad_lines='skip')\n"
          ]
        }
      ],
      "source": [
        "path = './data.tsv'\n",
        "# path = '/content/drive/MyDrive/CSCI544/amazon_reviews_us_Office_Products_v1_00.tsv'\n",
        "# read raw data set\n",
        "raw_data = pd.read_table(path, on_bad_lines='skip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "61YMLd7TiKkv",
        "outputId": "54f72c55-3114-4b05-c36e-a73a217978df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-db5226ba4bb8>:10: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(review, 'html.parser')\n",
            "<ipython-input-4-db5226ba4bb8>:10: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  soup = BeautifulSoup(review, 'html.parser')\n"
          ]
        }
      ],
      "source": [
        "# data cleaning and preprocessing\n",
        "new_data = raw_data[['star_rating', 'review_body']].copy()\n",
        "\n",
        "# Lowercase\n",
        "new_data['review_body'] = new_data['review_body'].str.lower()\n",
        "\n",
        "# remove the HTML and URLs from the reviews\n",
        "def remove_html(review):\n",
        "    if isinstance(review, str):\n",
        "        soup = BeautifulSoup(review, 'html.parser')\n",
        "        text = soup.get_text()\n",
        "        return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    else:\n",
        "        return review\n",
        "\n",
        "new_data['review_body'] = new_data['review_body'].apply(remove_html)\n",
        "\n",
        "# remove non-alphabetical characters\n",
        "new_data['review_body'] = new_data['review_body'].str.replace(r'[^a-z\\s]', '',regex=True)\n",
        "\n",
        "# Remove numbers and floats\n",
        "def remove_float(string):\n",
        "    return re.sub(r'[-+]?\\d*\\.\\d+|\\d+', '', str(string))\n",
        "new_data['review_body'] = new_data['review_body'].apply(remove_float)\n",
        "\n",
        "# remove extra spaces\n",
        "def remove_spaces(string):\n",
        "    return ' '.join(string.split())\n",
        "\n",
        "new_data['review_body'] = new_data['review_body'].apply(remove_spaces)\n",
        "\n",
        "# remove null values\n",
        "new_data = new_data.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CpW0XBbGkjgh"
      },
      "outputs": [],
      "source": [
        "new_data['class'] = 0\n",
        "# Let ratings with the values of 1, 2 and 3 form class 1\n",
        "new_data.loc[new_data['star_rating'].isin([1, 2, 3]), 'class'] = 1\n",
        "# Let ratings with the values of 4 and 5 form class 2\n",
        "new_data.loc[new_data['star_rating'].isin([4, 5]), 'class'] = 2\n",
        "# Delete entries with invalid ratings\n",
        "new_data = new_data[new_data['class'] != 0]\n",
        "\n",
        "# randomly select 50000 reviews from each class to build a balanced dataset of 100K reviews\n",
        "class1 = new_data[new_data['class'] == 1].sample(n = 50000, random_state=7, ignore_index=True)\n",
        "class2 = new_data[new_data['class'] == 2].sample(n = 50000, random_state=7, ignore_index=True)\n",
        "samples = pd.concat([class1, class2], axis=0).reset_index(drop=True)\n",
        "\n",
        "# # split data into training and testing dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(samples['review_body'], samples['class'],\n",
        "                                                    test_size=0.2, random_state=7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLWVo2fjllmX"
      },
      "source": [
        "# 2. Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "E3RXwsMOhNHU"
      },
      "outputs": [],
      "source": [
        "# Load the pretrained “word2vec-google-news-300” Word2Vec model\n",
        "import gensim.downloader as api\n",
        "wv = api.load('word2vec-google-news-300')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHK39ofMu-QO"
      },
      "source": [
        "(a) check semantic similarities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ5G6Zb16Jef"
      },
      "source": [
        "first example: sky - cloud + sun ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "H2FIFIvti4j1",
        "outputId": "ab7b4ab6-46d9-40ed-b067-7cdc2abac391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words similar to 'sky - cloud + sun': [('sunlight', 0.528194010257721)]\n"
          ]
        }
      ],
      "source": [
        "similar_word = wv.most_similar(positive=['sky', 'sun'], negative=['cloud'], topn=1)\n",
        "print(f\"Words similar to 'sky - cloud + sun': {similar_word}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6TtDx9P6NjP"
      },
      "source": [
        "second example: children~kids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LeHjm44QvP6I",
        "outputId": "53b6973c-2f59-492a-ae37-4c78361d4120"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity score between 'children' and 'kids': 0.743905\n"
          ]
        }
      ],
      "source": [
        "similarity_score = wv.similarity('children', 'kids')\n",
        "print(\"Similarity score between 'children' and 'kids':\", similarity_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoKyDsds9AkH"
      },
      "source": [
        "thrid example: beautiful~gorgeous"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qnfH3NuU9KZQ",
        "outputId": "85493f32-6381-4f4e-8f3a-7df5cac07109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity score between 'beautiful' and 'gorgeous': 0.8353004\n"
          ]
        }
      ],
      "source": [
        "similarity_score = wv.similarity('beautiful', 'gorgeous')\n",
        "print(\"Similarity score between 'beautiful' and 'gorgeous':\", similarity_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h77PKJEt9tPv"
      },
      "source": [
        "(b) Train a Word2Vec model using your own dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3_8Xa4wl9t_M"
      },
      "outputs": [],
      "source": [
        "# tokenize data\n",
        "tokenized_review = samples['review_body'].apply(word_tokenize)\n",
        "\n",
        "my_w2v = Word2Vec(tokenized_review, vector_size=300, window=13, min_count=9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T0zVQyhBS6U"
      },
      "source": [
        "Check the semantic similarities for the same two examples in part (a)\n",
        "\n",
        "first example: sky - cloud + sun ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8RfUIFCcBTUu",
        "outputId": "d025f3dd-ec2e-4261-db8a-4489cd3e8b65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words similar to 'sky - cloud + sun': [('glow', 0.6744972467422485)]\n"
          ]
        }
      ],
      "source": [
        "similar_word = my_w2v.wv.most_similar(positive=['sky', 'sun'], negative=['cloud'], topn=1)\n",
        "print(f\"Words similar to 'sky - cloud + sun': {similar_word}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aJSeYLeBYG_"
      },
      "source": [
        "second example: beautiful~gorgeous"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "T8xRAghCBqxi",
        "outputId": "798f8e98-cfc2-4ca9-a4a7-574947e0c25f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity score between 'beautiful' and 'gorgeous': 0.7948074\n"
          ]
        }
      ],
      "source": [
        "similarity_score = my_w2v.wv.similarity('beautiful', 'gorgeous')\n",
        "print(\"Similarity score between 'beautiful' and 'gorgeous':\", similarity_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxXyYvqqDPKw"
      },
      "source": [
        "What do you conclude from comparing vectors generated by yourself and the pretrained model?\n",
        "\n",
        "\n",
        "> Both your custom model and the pretrained model show a high similarity score between 'beautiful' and 'gorgeous', while my model gives a similar score of 0.818 and the pretrained model gives 0.835. This suggests that both models have learned a similar relationship between these two words, indicating that they are semantically similar in the context they were trained on. For words similar to 'sun - cloud + sun', my model outputs '[('glow', 0.6235448122024536)]' and the pretrained model outputs '[('sunlight', 0.528194010257721)].'  In both cases, the results make some sense in the context of the operation, even though they are not exactly the same. Overall, the results show that both models are capable of capturing semantic relationships between words. However, the pretrained model performs slightly better because of its vast and diverse scale of training data.\n",
        "\n",
        "Which of the Word2Vec models seems to encode semantic similarities between words better?\n",
        "\n",
        "\n",
        "> The Google pre-trained Word2Vec model seems to encode semantic similarities between words better. In the example of 'sky - cloud + sun', the Google pre-trained Word2Vec generates 'sunlight', while my model generates 'glow'. In the given context, 'sunlight' will be an answer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW28pHOkDcXB"
      },
      "source": [
        "#3. Simple models\n",
        "\n",
        "generate vectors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_oitxC_WDd1A"
      },
      "outputs": [],
      "source": [
        "# calculate the average Word2Vec vectors for each review\n",
        "def getVectors(sentence):\n",
        "    tokenized = word_tokenize(sentence)\n",
        "    num_words = 0\n",
        "    total = np.zeros(300)\n",
        "    for word in tokenized:\n",
        "        if word in wv:\n",
        "            total += wv[word]\n",
        "            num_words += 1\n",
        "\n",
        "    if num_words > 0:\n",
        "        average_vector = total / num_words\n",
        "    else:\n",
        "        average_vector = np.zeros(300)\n",
        "    return average_vector\n",
        "\n",
        "\n",
        "X_train_w2v = np.array([getVectors(review) for review in X_train])\n",
        "X_test_w2v = np.array([getVectors(review) for review in X_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rhc96ADGh-Cm"
      },
      "outputs": [],
      "source": [
        "# TF-IDF Feature Extraction\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_XR-DVchfUJ"
      },
      "source": [
        "perceptron model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "OSTBTYuZcw4c",
        "outputId": "1b5027b7-3553-4f61-b4ac-7c317a1fb2c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Report for training Perceptron model using word2vec features:\n",
            "Accuracy: 0.799.\n",
            "Report for training Perceptron model using TFIDF features:\n",
            "Accuracy: 0.808.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "perceptron = Perceptron(random_state=6)\n",
        "\n",
        "# train a perceptron model using word2vec feature\n",
        "perceptron.fit(X_train_w2v, y_train)\n",
        "preceptron_predict = perceptron.predict(X_test_w2v)\n",
        "\n",
        "# Report Precision, Recall, and f1-score for word2vec features.\n",
        "w2v_accuracy = accuracy_score(y_test, preceptron_predict)\n",
        "# w2v_precision = precision_score(y_test, preceptron_predict)\n",
        "# w2v_recall = recall_score(y_test, preceptron_predict)\n",
        "# w2v_f1 = f1_score(y_test, preceptron_predict)\n",
        "\n",
        "print('Report for training Perceptron model using word2vec features:')\n",
        "# print(f'Precision: {w2v_precision:.3f}; Recall: {w2v_recall:.3f}; F1: {w2v_f1:.3f}')\n",
        "print(f'Accuracy: {w2v_accuracy:.3f}.')\n",
        "\n",
        "# train a perceptron model for tfidf\n",
        "perceptron.fit(X_train_tfidf, y_train)\n",
        "preceptron_predict = perceptron.predict(X_test_tfidf)\n",
        "\n",
        "# Report Precision, Recall, and f1-score\n",
        "tfidf_accuracy = accuracy_score(y_test, preceptron_predict)\n",
        "# tfidf_precision = precision_score(y_test, preceptron_predict)\n",
        "# tfidf_recall = recall_score(y_test, preceptron_predict)\n",
        "# tfidf_f1 = f1_score(y_test, preceptron_predict)\n",
        "\n",
        "print('Report for training Perceptron model using TFIDF features:')\n",
        "# print(f'Precision: {tfidf_precision:.3f}; Recall: {tfidf_recall:.3f}; F1: {tfidf_f1:.3f}')\n",
        "print(f'Accuracy: {tfidf_accuracy:.3f}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY2exyo3hh9H"
      },
      "source": [
        "SVM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "TfxFqH9EeO3A",
        "outputId": "d0fa0b07-370c-4bf2-a9a8-0d7a0e65729b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Report for training SVM model using word2vec features:\n",
            "Accuracy: 0.816.\n",
            "Report for training SVM model using TFIDF features:\n",
            "Accuracy: 0.866.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svm_model = LinearSVC(random_state=6, max_iter=100000, tol=1e-6, dual=True)\n",
        "# train svm model using word2vec features\n",
        "svm_model.fit(X_train_w2v, y_train)\n",
        "svm_pred = svm_model.predict(X_test_w2v)\n",
        "\n",
        "# Report Precision, Recall, and f1-score for training SVM using word2vec features.\n",
        "w2v_accuracy = accuracy_score(y_test, svm_pred)\n",
        "# w2v_precision = precision_score(y_test, svm_pred)\n",
        "# w2v_recall = recall_score(y_test, svm_pred)\n",
        "# w2v_f1 = f1_score(y_test, svm_pred)\n",
        "\n",
        "print('Report for training SVM model using word2vec features:')\n",
        "# print(f'Precision: {w2v_precision:.3f}; Recall: {w2v_recall:.3f}; F1: {w2v_f1:.3f}')\n",
        "print(f'Accuracy: {w2v_accuracy:.3f}.')\n",
        "\n",
        "# train svm model using tfidf features\n",
        "svm_model.fit(X_train_tfidf, y_train)\n",
        "svm_pred = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "# Report Precision, Recall, and f1-score for training SVM using tfidf features.\n",
        "tfidf_accuracy = accuracy_score(y_test, svm_pred)\n",
        "tfidf_precision = precision_score(y_test, svm_pred)\n",
        "tfidf_recall = recall_score(y_test, svm_pred)\n",
        "tfidf_f1 = f1_score(y_test, svm_pred)\n",
        "\n",
        "print('Report for training SVM model using TFIDF features:')\n",
        "# print(f'Precision: {tfidf_precision:.3f}; Recall: {tfidf_recall:.3f}; F1: {tfidf_f1:.3f}')\n",
        "print(f'Accuracy: {tfidf_accuracy:.3f}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU9n3xxvnUIP"
      },
      "source": [
        "What do you conclude from comparing performances for the models trained using the two different feature types?\n",
        "\n",
        "\n",
        ">The accuracy values for both model have shown that the models trained using TFIDF features have a slightly better performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv-CdkTNE6Wz"
      },
      "source": [
        "#4. Feedforward Neural Networks\n",
        "\n",
        "(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zKwY5N4MJpn"
      },
      "source": [
        "prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "g43lRX6TMMj4"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define custom dataset class\n",
        "class dataset(Dataset):\n",
        "  def __init__(self,x,y):\n",
        "    self.x = torch.tensor(x,dtype=torch.float32)\n",
        "    self.y = torch.tensor(y,dtype=torch.int64) - 1\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    return self.x[idx],self.y[idx]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "HRppS5P-Qu-f"
      },
      "outputs": [],
      "source": [
        "y_test = y_test.to_numpy()\n",
        "\n",
        "trainset = dataset(X_train_w2v, y_train)\n",
        "testset = dataset(X_test_w2v, y_test)\n",
        "\n",
        "#DataLoader\n",
        "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6PQ43FSMfz_"
      },
      "source": [
        "create the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "R2KnYtvWLCIz"
      },
      "outputs": [],
      "source": [
        "# Define a feedforward neural network architechture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(Net, self).__init__()\n",
        "        # number of hidden nodes in each layer\n",
        "        hidden_1 = 50\n",
        "        hidden_2 = 5\n",
        "        self.fc1 = nn.Linear(input_size, hidden_1)\n",
        "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
        "        # output layer\n",
        "        self.output = nn.Linear(hidden_2, 2)\n",
        "        # define drop out layer\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # add hidden layer, with relu activation function\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mjrTrF5NAdB"
      },
      "source": [
        "train the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NeNygfpJE7hw",
        "outputId": "77f662a7-69cf-451f-d395-6b8de61c64ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.6795\n",
            "Epoch [2/20], Loss: 0.5770\n",
            "Epoch [3/20], Loss: 0.4137\n",
            "Epoch [4/20], Loss: 0.3509\n",
            "Epoch [5/20], Loss: 0.6418\n",
            "Epoch [6/20], Loss: 0.3148\n",
            "Epoch [7/20], Loss: 0.2354\n",
            "Epoch [8/20], Loss: 0.3913\n",
            "Epoch [9/20], Loss: 0.3925\n",
            "Epoch [10/20], Loss: 0.3883\n",
            "Epoch [11/20], Loss: 0.2705\n",
            "Epoch [12/20], Loss: 0.3290\n",
            "Epoch [13/20], Loss: 0.4074\n",
            "Epoch [14/20], Loss: 0.4191\n",
            "Epoch [15/20], Loss: 0.2520\n",
            "Epoch [16/20], Loss: 0.2000\n",
            "Epoch [17/20], Loss: 0.3561\n",
            "Epoch [18/20], Loss: 0.2708\n",
            "Epoch [19/20], Loss: 0.4137\n",
            "Epoch [20/20], Loss: 0.2196\n"
          ]
        }
      ],
      "source": [
        "# initiate model\n",
        "FNNa = Net(300).to(device)\n",
        "\n",
        "# specify loss function (categorical cross-entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# specify optimizer\n",
        "learning_rate = 0.001\n",
        "# optimizer = torch.optim.SGD(FNNa.parameters(), lr=learning_rate)\n",
        "optimizer = torch.optim.Adam(FNNa.parameters(), lr=learning_rate)\n",
        "\n",
        "# training the network\n",
        "# number of epochs to train the model\n",
        "n_epochs = 20\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    FNNa.train()\n",
        "    for data, target in trainloader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = FNNa(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print loss for every epoch\n",
        "    print(f\"Epoch [{epoch + 1}/{n_epochs}], Loss: {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aamxnn5vNW_l"
      },
      "source": [
        "make prediction on test set and report accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8zDbqYftwOtu",
        "outputId": "a6e6fc9b-dc55-4ded-e6c7-d49976d4c06d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Report for training Feedforward Neural Network using the average Word2Vec vectors:\n",
            "Accuracy: 0.839.\n"
          ]
        }
      ],
      "source": [
        "def eval(model, dataloader):\n",
        "    prediction_list = []\n",
        "    true_list = []\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        x = x.to(device)\n",
        "        outputs = model(x)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        prediction_list.extend(predicted.cpu().numpy())\n",
        "        true_list.extend(y.numpy())\n",
        "    return accuracy_score(true_list, prediction_list)\n",
        "\n",
        "w2v_accuracy = eval(FNNa,testloader)\n",
        "print('Report for training Feedforward Neural Network using the average Word2Vec vectors:')\n",
        "print(f'Accuracy: {w2v_accuracy:.3f}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--pmxQAhFuV4"
      },
      "source": [
        "(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkBIvXxh6KQA"
      },
      "source": [
        "generate input vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "AQ8oPd6SFvfi"
      },
      "outputs": [],
      "source": [
        "# calculate the first 10 Word2Vec vectors for each review\n",
        "def getTenVectors(sentence):\n",
        "    tokenized = word_tokenize(sentence)\n",
        "    total = np.array([])\n",
        "    count = 0\n",
        "    for word in tokenized:\n",
        "        if count < 10:\n",
        "            if word in wv:\n",
        "                total = np.concatenate((total, wv[word]))\n",
        "            else:\n",
        "                total = np.concatenate((total, np.zeros(300)))\n",
        "            count += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    while len(total) < 3000:\n",
        "         total = np.concatenate((total, np.zeros(300)))\n",
        "    return total\n",
        "\n",
        "X_train_10_w2v = np.array([getTenVectors(review) for review in X_train])\n",
        "X_test_10_w2v = np.array([getTenVectors(review) for review in X_test])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxNw2DPVNhX_"
      },
      "source": [
        "prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ztWtJb5mNio5"
      },
      "outputs": [],
      "source": [
        "trainset = dataset(X_train_10_w2v, y_train)\n",
        "testset = dataset(X_test_10_w2v, y_test)\n",
        "\n",
        "#DataLoader\n",
        "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFRBh97rKNI1"
      },
      "source": [
        "train the network\n",
        "\n",
        "We can use the same Net class defined in part (a) to build a feedforward neural network, but for part (b), the input size is changed to 3000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "q8wvaBDqFv7w",
        "outputId": "5991f40a-8a32-4249-ebfe-b4ce2bb7dfad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.5302\n",
            "Epoch [2/20], Loss: 0.4463\n",
            "Epoch [3/20], Loss: 0.5834\n",
            "Epoch [4/20], Loss: 0.5409\n",
            "Epoch [5/20], Loss: 0.4429\n",
            "Epoch [6/20], Loss: 0.2793\n",
            "Epoch [7/20], Loss: 0.2628\n",
            "Epoch [8/20], Loss: 0.2073\n",
            "Epoch [9/20], Loss: 0.1963\n",
            "Epoch [10/20], Loss: 0.3585\n",
            "Epoch [11/20], Loss: 0.1140\n",
            "Epoch [12/20], Loss: 0.0749\n",
            "Epoch [13/20], Loss: 0.0960\n",
            "Epoch [14/20], Loss: 0.1955\n",
            "Epoch [15/20], Loss: 0.0642\n",
            "Epoch [16/20], Loss: 0.1241\n",
            "Epoch [17/20], Loss: 0.1422\n",
            "Epoch [18/20], Loss: 0.0950\n",
            "Epoch [19/20], Loss: 0.0892\n",
            "Epoch [20/20], Loss: 0.1255\n"
          ]
        }
      ],
      "source": [
        "# initiate the network\n",
        "FNNb = Net(3000).to(device)\n",
        "\n",
        "# specify loss function (categorical cross-entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# specify optimizer\n",
        "learning_rate = 0.001\n",
        "# optimizer = torch.optim.SGD(FNNa.parameters(), lr=learning_rate)\n",
        "optimizer = torch.optim.Adam(FNNb.parameters(), lr=learning_rate)\n",
        "\n",
        "# training the network\n",
        "# number of epochs to train the model\n",
        "n_epochs = 20\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    FNNb.train()\n",
        "    for data, target in trainloader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = FNNb(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print loss for every epoch\n",
        "    print(f\"Epoch [{epoch + 1}/{n_epochs}], Loss: {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNPjyeaKOFyB"
      },
      "source": [
        "make prediction on test set and report accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "49mjyYkGOKRf",
        "outputId": "92902fa0-923a-4871-94ce-3d066ff54e92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Report for training Feedforward Neural Network using the first 10 Word2Vec vectors:\n",
            "Accuracy: 0.731.\n"
          ]
        }
      ],
      "source": [
        "w2v_accuracy = eval(FNNb,testloader)\n",
        "print('Report for training Feedforward Neural Network using the first 10 Word2Vec vectors:')\n",
        "print(f'Accuracy: {w2v_accuracy:.3f}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLD2pv06UPFl"
      },
      "source": [
        "What do you conclude by comparing accuracy values you obtain with\n",
        "those obtained in the “Simple Models” section.\n",
        "\n",
        "\n",
        "> The feedforward neural network trained using the average Word2Vec vectors has a better accuracy value compared to both models in the “Simple Models” section. Thus, with the same training data, feedforward neural network has a better performance over the “Simple Models”. However, the feedforward neural network trained using the first 10 Word2Vec vectors perform worse than both models in the “Simple Models” section.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chPgL67uFwHI"
      },
      "source": [
        "#5. Recurrent Neural Networks\n",
        "\n",
        "(a)\n",
        "\n",
        "prepare data\n",
        "\n",
        "For preparing the data, this part will be using the custom dataset class defined in part 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "RGQUQOBfF2Ey"
      },
      "outputs": [],
      "source": [
        "def truncate(sentence):\n",
        "    tokenized = word_tokenize(sentence)\n",
        "    total = np.zeros((10,300))\n",
        "    for i, word in enumerate(tokenized):\n",
        "        if i >= 10:\n",
        "            break\n",
        "        if word in wv:\n",
        "          total[i] = wv[word]\n",
        "\n",
        "    return total\n",
        "\n",
        "X_train_rnn = np.array([truncate(review) for review in X_train])\n",
        "X_test_rnn = np.array([truncate(review) for review in X_test])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = dataset(X_train_rnn, y_train)\n",
        "testset = dataset(X_test_rnn, y_test)\n",
        "\n",
        "#DataLoader\n",
        "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=1, shuffle=False)\n"
      ],
      "metadata": {
        "id": "WtyFrscTx1tZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "define model architecture"
      ],
      "metadata": {
        "id": "J6a4m0bpfjbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        # rnn layer\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, nonlinearity='relu', dropout=0.2, batch_first=True)\n",
        "        # output layer\n",
        "        self.output = nn.Linear(hidden_size, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state with zeros\n",
        "        h0 = torch.zeros(1, x.size(0), 10).to(device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.output(out[:, -1, :])\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "h1PfroDofhGE"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train the network"
      ],
      "metadata": {
        "id": "BrJ-CZyR7Wer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 10\n",
        "RNN = SimpleRNN(300, hidden_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(RNN.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the network\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (x, y) in enumerate(trainloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        outputs = RNN(x)\n",
        "        loss = criterion(outputs, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "X4GO-KOLQJLA",
        "outputId": "b9d440dc-111e-4ed8-b3d0-d66e7cb86cb8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.6175\n",
            "Epoch [2/20], Loss: 0.4481\n",
            "Epoch [3/20], Loss: 0.4493\n",
            "Epoch [4/20], Loss: 0.4393\n",
            "Epoch [5/20], Loss: 0.4596\n",
            "Epoch [6/20], Loss: 0.4626\n",
            "Epoch [7/20], Loss: 0.3558\n",
            "Epoch [8/20], Loss: 0.5406\n",
            "Epoch [9/20], Loss: 0.4424\n",
            "Epoch [10/20], Loss: 0.4309\n",
            "Epoch [11/20], Loss: 0.3818\n",
            "Epoch [12/20], Loss: 0.3864\n",
            "Epoch [13/20], Loss: 0.5633\n",
            "Epoch [14/20], Loss: 0.4773\n",
            "Epoch [15/20], Loss: 0.4252\n",
            "Epoch [16/20], Loss: 0.3958\n",
            "Epoch [17/20], Loss: 0.5686\n",
            "Epoch [18/20], Loss: 0.6625\n",
            "Epoch [19/20], Loss: 0.4233\n",
            "Epoch [20/20], Loss: 0.3666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "make prediction on test set and report accuracy score\n",
        "\n",
        "For model evaluation and reporting accuracy value, this part will be using the same eval function defined in part 4 to generate the accuracy score for each model."
      ],
      "metadata": {
        "id": "l6X7a1k87Xix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_accuracy = eval(RNN,testloader)\n",
        "\n",
        "print('Report for training Recurrent Neural Network using the average Word2Vec vectors:')\n",
        "print(f'Accuracy: {rnn_accuracy:.3f}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "upQ4PIzkSuA6",
        "outputId": "c48e1c0c-f6e9-4776-f9c8-43e21a3223be"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Report for training Recurrent Neural Network using the average Word2Vec vectors:\n",
            "Accuracy: 0.780.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go6EmCGpVobV"
      },
      "source": [
        "What do you conclude by comparing accuracy values you obtain with those obtained with feedforward neural network models.\n",
        "\n",
        "\n",
        "> After comparing the accuracy values, it is shown that for models trained using the same average word2vec vectors, the Feedforward Neural Network has a better performance. The RNN network performs better than the FNN that is trained on the first 10 word vectors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8R9rYYxVcj2"
      },
      "source": [
        "(b)\n",
        "\n",
        "model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "K2XisM5QVdOR"
      },
      "outputs": [],
      "source": [
        "class GRUNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(GRUNetwork, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True, dropout=0.1)\n",
        "        self.output = nn.Linear(hidden_size, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(device)\n",
        "        out, _ = self.gru(x, h0)\n",
        "        out = self.output(self.relu(out[:, -1, :]))\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train the network"
      ],
      "metadata": {
        "id": "ty6Uu360UoE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 10\n",
        "GRU = GRUNetwork(300, hidden_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(GRU.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the network\n",
        "num_epochs = 30\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (x, y) in enumerate(trainloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs = GRU(x)\n",
        "        loss = criterion(outputs, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "AeeEZI-kUnXT",
        "outputId": "98db0126-f8ae-4fbc-8b9c-da6f8e22d441"
      },
      "execution_count": 32,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Loss: 0.5030\n",
            "Epoch [2/30], Loss: 0.5049\n",
            "Epoch [3/30], Loss: 0.4333\n",
            "Epoch [4/30], Loss: 0.4125\n",
            "Epoch [5/30], Loss: 0.5265\n",
            "Epoch [6/30], Loss: 0.8146\n",
            "Epoch [7/30], Loss: 0.5095\n",
            "Epoch [8/30], Loss: 0.3355\n",
            "Epoch [9/30], Loss: 0.3028\n",
            "Epoch [10/30], Loss: 0.3275\n",
            "Epoch [11/30], Loss: 0.3502\n",
            "Epoch [12/30], Loss: 0.4136\n",
            "Epoch [13/30], Loss: 0.4535\n",
            "Epoch [14/30], Loss: 0.3637\n",
            "Epoch [15/30], Loss: 0.5132\n",
            "Epoch [16/30], Loss: 0.4883\n",
            "Epoch [17/30], Loss: 0.2734\n",
            "Epoch [18/30], Loss: 0.4661\n",
            "Epoch [19/30], Loss: 0.3088\n",
            "Epoch [20/30], Loss: 0.5329\n",
            "Epoch [21/30], Loss: 0.1946\n",
            "Epoch [22/30], Loss: 0.3369\n",
            "Epoch [23/30], Loss: 0.4002\n",
            "Epoch [24/30], Loss: 0.3482\n",
            "Epoch [25/30], Loss: 0.5557\n",
            "Epoch [26/30], Loss: 0.2849\n",
            "Epoch [27/30], Loss: 0.3148\n",
            "Epoch [28/30], Loss: 0.3150\n",
            "Epoch [29/30], Loss: 0.3127\n",
            "Epoch [30/30], Loss: 0.2642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "make prediction on test set and report accuracy score"
      ],
      "metadata": {
        "id": "8duFUHOfcxJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gru_accuracy = eval(GRU,testloader)\n",
        "\n",
        "print('Report for training Recurrent Neural Network with gated recurrent unit cell using the average Word2Vec vectors:')\n",
        "\n",
        "print(f'Accuracy: {gru_accuracy:.3f}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7NqNAR0Bc20m",
        "outputId": "7f73480f-f9fe-4c18-d6b7-de9c4d56e26d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Report for training Recurrent Neural Network with gated recurrent unit cell using the average Word2Vec vectors:\n",
            "Accuracy: 0.790.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKudgYmwVd20"
      },
      "source": [
        "(c)\n",
        "\n",
        "model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "otv3rJoeVeZf"
      },
      "outputs": [],
      "source": [
        "class LSTMNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(LSTMNetwork, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, dropout=0.2)\n",
        "        self.output = nn.Linear(hidden_size, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0,c0))\n",
        "        out = self.output(self.relu(out[:, -1, :]))\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train the network"
      ],
      "metadata": {
        "id": "oEX5yELGYo8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 10\n",
        "LSTM = LSTMNetwork(300, hidden_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(LSTM.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the network\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (x, y) in enumerate(trainloader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs = LSTM(x)\n",
        "        loss = criterion(outputs, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZqRCfD5wYoZT",
        "outputId": "7df37075-18aa-4161-ec52-9bef54e2d250"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.4985\n",
            "Epoch [2/20], Loss: 0.3211\n",
            "Epoch [3/20], Loss: 0.4579\n",
            "Epoch [4/20], Loss: 0.3009\n",
            "Epoch [5/20], Loss: 0.4059\n",
            "Epoch [6/20], Loss: 0.4050\n",
            "Epoch [7/20], Loss: 0.3370\n",
            "Epoch [8/20], Loss: 0.4888\n",
            "Epoch [9/20], Loss: 0.4017\n",
            "Epoch [10/20], Loss: 0.3831\n",
            "Epoch [11/20], Loss: 0.3271\n",
            "Epoch [12/20], Loss: 0.5342\n",
            "Epoch [13/20], Loss: 0.1899\n",
            "Epoch [14/20], Loss: 0.4108\n",
            "Epoch [15/20], Loss: 0.3435\n",
            "Epoch [16/20], Loss: 0.5865\n",
            "Epoch [17/20], Loss: 0.2926\n",
            "Epoch [18/20], Loss: 0.3920\n",
            "Epoch [19/20], Loss: 0.2663\n",
            "Epoch [20/20], Loss: 0.3652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "make prediction on test set and report accuracy score"
      ],
      "metadata": {
        "id": "eiaXi9tIYsEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_accuracy = eval(LSTM, testloader)\n",
        "\n",
        "print('Report for training Recurrent Neural Network with LSTM unit cell using the average Word2Vec vectors:')\n",
        "\n",
        "print(f'Accuracy: {lstm_accuracy:.3f}.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ow8fUgunYrkr",
        "outputId": "5e74df67-57a4-4857-bc0e-6617a01e7e8b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Report for training Recurrent Neural Network with LSTM unit cell using the average Word2Vec vectors:\n",
            "Accuracy: 0.796.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpK8XR_WVr5g"
      },
      "source": [
        "What do you conclude by comparing accuracy values you obtain by GRU,\n",
        "LSTM, and simple RNN.\n",
        "\n",
        "\n",
        "> The accuracy scores reveal that LSTM outperforms the other two models, with GRU coming in second, and RNN trailing as the least accurate. It's worth noting that their accuracy scores are quite similar, all hovering around the 0.79 mark. As a result, I would like to emphasize that the performance of these three models is relatively consistent, with LSTM showing a slight advantage.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}